# HR Tech Portfolio
---
> ğŸ§‘â€ğŸ’» HR Tech & People Analytics Portfolio â€” from descriptive insights â†’ predictive models â†’ explainable AI.  
> ğŸŒ Live Demo: 
[AttritionDashboard](https://hr-tech-portfolio.streamlit.app/)
---
![PDF Available](https://img.shields.io/badge/PDF-Available-brightgreen?logo=adobeacrobatreader)
---

## ğŸ“‘ Reports Panel

[![Download Latest Report](https://img.shields.io/badge/PDF-Download%20Latest-brightgreen?style=for-the-badge&logo=adobeacrobatreader)](https://github.com/AMBOT-pixel96/hr-tech-portfolio/raw/main/reports/Attrition_Project_Summary.pdf?download=1)
[![View in Repo](https://img.shields.io/badge/View-Reports-blue?style=for-the-badge&logo=github)](reports/)
[![Download All](https://img.shields.io/badge/ZIP-Download%20All-orange?style=for-the-badge&logo=files)](https://github.com/AMBOT-pixel96/hr-tech-portfolio/archive/refs/heads/main.zip)

---

![Python](https://img.shields.io/badge/Python-3.9-blue)  
![Jupyter Notebook](https://img.shields.io/badge/Notebook-Jupyter-orange)  
![Pandas](https://img.shields.io/badge/Library-Pandas-150458?logo=pandas)  
![Seaborn](https://img.shields.io/badge/Library-Seaborn-3776AB)  
![Matplotlib](https://img.shields.io/badge/Library-Matplotlib-11557c)  
![Dataset](https://img.shields.io/badge/Dataset-IBM%20HR%20Analytics-green)  

This repository showcases my journey into **HR Tech & People Analytics**.

This repository showcases my journey into **HR Tech & People Analytics**.  
It contains hands-on projects where I apply **Python, Pandas, Seaborn, and People Analytics concepts** to real-world HR data.

---

## ğŸ“‹ Project Overview  

| Project | Notebook | Objective | Key Results |
|---------|----------|------------|-------------|
| **1. Attrition Risk Analyzer (v1.0)** | [Day4-AttritionRiskAnalyzer.ipynb](notebooks/Day4-AttritionRiskAnalyzer.ipynb) | Descriptive analytics of attrition patterns (age, dept, job role) | Attrition rate **16.1%**, highest in Sales Reps (39%) |
| **2. Predictive Attrition Model (Logistic v2.0 / v3.0)** | [Attrition_PredictiveModel_V2.ipynb](notebooks/Attrition_PredictiveModel_V2.ipynb)<br>[Attrition_PredictiveModel_V3.ipynb](notebooks/Attrition_PredictiveModel_V3.ipynb) | Baseline + tuned Logistic Regression | Accuracy ~**75%**, ROC AUC ~**0.80**, key drivers: Overtime, JobRole, Marital Status |
| **3. HR Data Cleaning Utility (Sidequest)** | [HR_Data_Cleaning_Utility_V1.ipynb](sidequests/HR_Data_Cleaning_Utility_V1.ipynb) | Cleaning messy HR data (duplicates, missing values, casing) | Automated pipeline â†’ [cleaned_hr_data.csv](data/cleaned_hr_data.csv) |
| **4. Model Comparison (Logistic vs Random Forest)** | [Attrition_ModelComparision.ipynb](notebooks/Attrition_ModelComparision.ipynb) | Compare interpretability vs non-linear power | Logistic: **75%** acc, RF: **83%** acc, RF captures income + age patterns |
| **5. Advanced Models (Tuned RF + XGBoost)** | [Attrition_AdvancedModels.ipynb](notebooks/Attrition_AdvancedModels.ipynb) | Hyperparameter tuned RF + XGBoost benchmarking | XGBoost best performer â†’ **86.4% acc**, ROC AUC **0.774** |
| **6. Explainability with SHAP** | [Attrition_ModelExplainability.ipynb](notebooks/Attrition_ModelExplainability.ipynb) | Add global & local interpretability (XGBoost) | Global drivers: Overtime, JobRole, Income; Local plots explain individuals |

---

## ğŸ“‚ Repository Structure  

<!-- REPO_TREE_START -->
```text
â”œâ”€â”€ README.md
â”œâ”€â”€ app.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ cleaned_hr_data.csv
â”‚   â”œâ”€â”€ logistic_top_features.csv
â”‚   â”œâ”€â”€ messy_hr_data.csv
â”‚   â”œâ”€â”€ model_comparison_results.csv
â”‚   â”œâ”€â”€ processed_hr_data.csv
â”‚   â”œâ”€â”€ rf_top_features.csv
â”‚   â””â”€â”€ xgb_top_features.csv
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ Attrition_by_dept.jpg
â”‚   â”œâ”€â”€ attrition_by_age.jpg
â”‚   â”œâ”€â”€ attrition_by_jobrole.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ confusion_matrix_xgb.png
â”‚   â”œâ”€â”€ missing_values_after.png
â”‚   â”œâ”€â”€ missing_values_before.png
â”‚   â”œâ”€â”€ missing_values_collage.png
â”‚   â”œâ”€â”€ model_comparision.png
â”‚   â”œâ”€â”€ model_comparison_barplot.png
â”‚   â”œâ”€â”€ roc_curve_comparison.png
â”‚   â”œâ”€â”€ screenshot1.png
â”‚   â”œâ”€â”€ shap_dependence_plot.png
â”‚   â”œâ”€â”€ shap_feature_importance.png
â”‚   â”œâ”€â”€ shap_local_bar_plot.png
â”‚   â”œâ”€â”€ shap_summary_plot.png
â”‚   â”œâ”€â”€ shap_waterfall_plot.png
â”‚   â”œâ”€â”€ top_features.png
â”‚   â”œâ”€â”€ top_features_post_enhancement.png
â”‚   â”œâ”€â”€ top_features_tuned.png
â”‚   â”œâ”€â”€ top_features_xgb.png
â”‚   â””â”€â”€ top_features_xgb_heatmap.png
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ Attrition_AdvancedModels.ipynb
â”‚   â”œâ”€â”€ logistic_attrition_model.pkl
â”‚   â”œâ”€â”€ logistic_attrition_model_tuned.pkl
â”‚   â”œâ”€â”€ placeholder
â”‚   â”œâ”€â”€ random_forest_attrition_model.pkl
â”‚   â”œâ”€â”€ random_forest_tuned.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”œâ”€â”€ trained_columns.pkl
â”‚   â””â”€â”€ xgboost_attrition_model.pkl
â”œâ”€â”€ models_joblib
â”‚   â””â”€â”€ test.txt
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€  Attrition_PredictiveModel.ipynb
â”‚   â”œâ”€â”€ Attrition_AdvancedModels.ipynb
â”‚   â”œâ”€â”€ Attrition_ModelComparision.ipynb
â”‚   â”œâ”€â”€ Attrition_ModelExplainability.ipynb
â”‚   â”œâ”€â”€ Attrition_PredictiveModel_V2.ipynb
â”‚   â”œâ”€â”€ Attrition_PredictiveModel_V3.ipynb
â”‚   â”œâ”€â”€ Attrition_Streamlit_Dashboard.ipynb
â”‚   â”œâ”€â”€ Day4-AttritionRiskAnalyzer.ipynb
â”‚   â”œâ”€â”€ Day4-AttritionRiskAnalyzer_v2.0.ipynb
â”‚   â”œâ”€â”€ baby_steps
â”‚   â”‚   â”œâ”€â”€ Day1-Hello-Amlan.ipynb
â”‚   â”‚   â”œâ”€â”€ Day2-Basics.ipynb
â”‚   â”‚   â””â”€â”€ Day3-DataTypesAndControl.ipynb
â”‚   â””â”€â”€ baby_steps.gitkeep
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ Attrition_AdvancedModels_aa41c1310f907082a92a9e51b94010b5.pdf
â”‚   â”œâ”€â”€ Attrition_ModelComparision_009086358233572ffb338cf8f4ee6ab7.pdf
â”‚   â”œâ”€â”€ Attrition_ModelExplainability_52d61660976d1481ab051617939779a2.pdf
â”‚   â”œâ”€â”€ Attrition_Project_Summary.pdf
â”‚   â”œâ”€â”€ Day4-AttritionRiskAnalyzer_v2.0_058b58b9bf5e96ac0c1bdaac1e7abdcc.pdf
â”‚   â”œâ”€â”€ HR_Data_Cleaning_Utility_V1_260622ce.pdf
â”‚   â”œâ”€â”€ HR_Data_Cleaning_Utility_V1_6501dfa8.pdf
â”‚   â”œâ”€â”€ HR_Data_Cleaning_Utility_V1_86b38b95.pdf
â”‚   â””â”€â”€ HR_Data_Cleaning_Utility_V1_87664d4cc3eefa67f9fe549d33ef7c57.pdf
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ runtime.txt
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ export_pdf.py
â”‚   â””â”€â”€ gen_tree.py
â””â”€â”€ sidequests
    â””â”€â”€ HR_Data_Cleaning_Utility_V1.ipynb
```
<!-- REPO_TREE_END -->

---

## ğŸ“Š Project 1: Attrition Risk Analyzer (v1.0)

### v1.0 â€” Descriptive Insights
**Objective:**  
Analyze IBMâ€™s HR Attrition dataset to identify patterns of employee attrition, create a risk flag, and visualize insights.

---

**Why This Matters (Business Context):**  
Employee attrition directly impacts business costs through lost productivity, rehiring, and retraining.  
By identifying high-risk roles and departments, organizations can proactively design retention strategies,  
reduce turnover costs, and improve workforce stability.

---

**Key Steps:**  
1. Load and explore the dataset (1470 employees, 35 features).  
2. Analyze attrition distribution overall and by job role.  
3. Create a binary attrition risk flag (Yes = 1, No = 0).  
4. Build visualizations:
   - Overall attrition counts  
   - Attrition by department  
   - Attrition by age distribution  
   - Attrition % by job role  

ğŸ““ [View the Jupyter Notebook](notebooks/Day4-AttritionRiskAnalyzer.ipynb)  

---

### v2.0 â€” Predictive Modeling (Logistic Regression)  

**Objective:**  
Move from descriptive analytics â†’ predictive insights by using **Logistic Regression** to forecast employee attrition risk. 

**Key Steps:**  
1. Data preprocessing  
   - Dropped irrelevant features (EmployeeNumber, Over18, etc.)  
   - Encoded categorical variables via one-hot encoding  
   - Scaled numeric features  
   - Train-test split (70/30)  
2. Model training with Logistic Regression (`scikit-learn`)  
3. Model evaluation  
   - Accuracy score  
   - Confusion matrix  
   - Classification report (precision, recall, F1)  
   - ROC-AUC score  
4. Feature importance analysis â†’ which factors most influence attrition risk  
5. Saved trained model + scaler into `/models/`  
6. Fixed Data Leakage issue and added enhancements
   
ğŸ““ [View the Predictive Notebook](notebooks/Attrition_PredictiveModel_V2.ipynb)  

---


## ğŸ–¼ï¸ Visuals & Outputs  

### Descriptive Analytics (v1.0)
# Attrition by Age:

![Attrition by Age](images/attrition_by_age.jpg)  

*Attrition counts (Yes/No) by Age*  

# Attrition by Department:  

![Attrition by Department](images/Attrition_by_dept.jpg)  

*Attrition counts (Yes/No) per Department*  

# Attrition % by Job Role:  

![Attrition % by Job Role](images/attrition_by_jobrole.png)  

*Percentage of employees leaving by Job Role*  

### Predictive Analytics (v2.0)  
![Confusion Matrix](images/confusion_matrix.png)  
*Confusion Matrix â€” Logistic Regression performance*  

![Top Features](images/top_features.png)  
*Top 10 features influencing attrition risk* 

![Top Features After Enhancement of Model](images/top_features_post_enhancement.png)  
*Top 10 features influencing attrition risk - Enhanced Model*

---

## ğŸ” Key Insights  

- Overall attrition rate: **16.1%**  
- Job roles with highest attrition:  
  - Sales Representatives â†’ 39% attrition (83 out of 220 left)  
  - Laboratory Technicians â†’ 23% attrition (62 out of 259 left)  
- Lowest attrition: **R&D (13.8%)** â†’ strongest retention  
- Highest attrition: **Sales (20.6%)** â†’ weakest retention  
- HR is small (63 employees), but attrition rate is relatively high (19%)  
- Age groups younger than ~30 show elevated risk compared to older cohorts  

**From Predictive Model:**  
- Logistic Regression achieved ~`75.9%` accuracy, ROC-AUC = `0.817`.  
- Key positive attrition drivers: *Overtime, JobRole_SalesRep, MaritalStatus_Single, etc.*  
- Key retention drivers: *JobLevel, YearsAtCompany, MonthlyIncome*. 

---
## Project 2- Predictive Attrition Model (v3.0) - Tuned Models

**Objective:**  
Enhance the baseline Logistic Regression model by applying **cross-validation** and **hyperparameter tuning** to improve stability and interpretability.  

**Key Steps:**  
1. Split dataset into training & test sets (80/20)  
2. Standardized numeric features with `StandardScaler`  
3. Trained baseline Logistic Regression (Day-5)  
4. Applied **5-fold cross-validation** to validate performance consistency  
5. Used **GridSearchCV** to tune hyperparameters (`C`, `penalty`, `solver`)  
6. Compared tuned vs baseline performance  

**Results:**  
- Baseline Accuracy: **XX%**  
- Tuned CV Accuracy: **XX%**  
- ROC AUC: **XX**  
- Best Parameters: `{ 'C': X, 'penalty': 'l1', 'solver': 'liblinear' }`  

**Sample Visuals:**  

Top Features Driving Attrition:  
![Top Features](images/top_features_tuned.png)  

**Insights:**  
- OverTime, Laboratory Technician roles, and Frequent Travel rank as top predictors.  
- Hyperparameter tuning improved model generalization, reducing overfitting risk.  
- Cross-validation confirmed stability of results across folds.
**Model Artifacts:**  
[logistic_top_features.csv](data/logistic_top_features.csv)
  
ğŸ““ [View the Predictive Notebook](notebooks/Attrition_PredictiveModel_V3.ipynb)

---

## ğŸ“Š Project 3: HR Data Cleaning Utility (v1.0) (Side Quest-1)

This notebook demonstrates how to **simulate messy HR data** and then build a cleaning pipeline to make it analysis-ready.  
Data cleaning is a critical step in People Analytics â€” poor quality data = misleading insights.
# âœ… Conclusions

- Automated pipeline successfully cleaned the dataset.  
- Issues fixed: duplicates, missing values, inconsistent casing, invalid dates.  
- Outputs saved in:
  - [messy_hr_data.csv](data/messy_hr_data.csv)  
  - [cleaned_hr_data.csv](data/cleaned_hr_data.csv)  

**Hero Visual: Data Cleaning Impact**  

![Before vs After Cleaning](images/missing_values_collage.png)

ğŸ““ [View the Utility Notebook](sidequests/HR_Data_Cleaning_Utility_V1.ipynb)  


## ğŸ“Š Project 4: Attrition Model Comparison (v3.0)

**Objective:**  
Benchmark **Logistic Regression** against a **Random Forest classifier** to see if tree-based models improve prediction of employee attrition.

**Key Steps:**  
1. Prepared dataset with clean features (no leakage)  
2. Trained baseline Logistic Regression (linear, interpretable)  
3. Trained Random Forest (non-linear, ensemble)  
4. Compared performance using Accuracy & ROC AUC  
5. Visualized confusion matrices and feature importance  

**Results:**  
- Logistic Regression â†’ Accuracy: **75%**, ROC AUC: **79%**  
- Random Forest â†’ Accuracy: **83%**, ROC AUC: **77%**  
- Random Forest showed stronger performance on non-linear features, while Logistic remains more interpretable.  

**Sample Visuals:**  
Confusion Matrix Comparison:  
![Model Comparison](images/model_comparision.png)  

Top Features (Logistic vs Random Forest):  
- Logistic: OverTime, SalesRep role, MaritalStatus=Single  
- Random Forest: OverTime, MonthlyIncome, Age buckets  
ğŸ““ [View the Model Comparison Notebook](notebooks/Attrition_ModelComparision.ipynb)

---

**Model Artifacts:**  
***Check Below for all Artifacts*** 
---

**Insights:**  
- Logistic = simple, transparent model (good for executive storytelling)  
- Random Forest = higher accuracy, captures complex patterns (good for prediction)  
- Next: try **XGBoost** and add **SHAP interpretability** for business-ready insights.

---

## ğŸ“Š Project 5: Advanced Attrition Models (Tuned RF + XGBoost)

**Objective:**  
Take predictive modeling beyond Logistic Regression by tuning Random Forest and introducing **XGBoost**, a gradient boosting algorithm widely used for structured/tabular datasets.  

---

**Key Steps:**  
1. Prepared dataset with cleaned features (no leakage).  
2. Trained and tuned Random Forest (grid search for depth, estimators, features).  
3. Trained XGBoost model with optimized hyperparameters.  
4. Compared model performance against Logistic Regression baseline.  
5. Visualized feature importances, ROC curves, and confusion matrices.  
6. Exported key data artifacts (top features, comparison metrics).  

---

**Results (Test Set):**  
- Logistic Regression â†’ Accuracy: **75.2%**, ROC AUC: **0.798**  
- Tuned Random Forest â†’ Accuracy: **83.7%**, ROC AUC: **0.769**  
- XGBoost â†’ Accuracy: **86.4%**, ROC AUC: **0.774**  

---

### ğŸ”¥ Sample Visuals  

**1. ROC Curve Comparison (Logistic vs RF vs XGBoost)**  
![ROC Curve Comparison](images/roc_curve_comparison.png)  
ğŸ“ˆ *Shows how Logistic, RF, and XGBoost trade off sensitivity vs specificity â€” XGBoost edges ahead on balance.*  

**2. Model Comparison (Accuracy vs ROC AUC)**  
![Model Comparison](images/model_comparison_barplot.png)  
ğŸ“Š *Side-by-side accuracy vs ROC AUC highlights overall performance differences across models.*  

**3. Top Features (XGBoost Heatmap)**  
![Top Features - XGBoost Heatmap](images/top_features_xgb_heatmap.png)  
ğŸ”¥ *Top 15 features by importance â€” OverTime, JobRole, and MonthlyIncome dominate attrition risk signals.*  

**4. Confusion Matrix (XGBoost)**  
![Confusion Matrix - XGBoost](images/confusion_matrix_xgb.png)  
ğŸ§© *Visual breakdown of predictions â€” where XGBoost gets it right (and where it misses).*  

---
### Notebook ğŸ““ [Attrition_AdvancedModels.ipynb â†’ Advanced Models (RF + XGBoost)](notebooks/Attrition_AdvancedModels.ipynb)  
---
### ğŸ“¦ Data Artifacts

- [rf_top_features.csv](data/rf_top_features.csv)  
- [xgb_top_features.csv](data/xgb_top_features.csv)  
- [model_comparison_results.csv](data/model_comparison_results.csv)
---
**Insights:**  
- Logistic Regression: strong interpretability but weaker predictive power.  
- Random Forest: higher accuracy (84%), robust to non-linearities.  
- XGBoost: best performer overall (86% accuracy), consistently picks up complex patterns (OverTime, MonthlyIncome, JobRole).  
---
**Next Steps:**  
- Add **SHAP interpretability** for XGBoost.  
- Deploy best model in a **Streamlit dashboard**.  
- Integrate SQL + ML for end-to-end HR analytics workflows.
---
## ğŸ“Š Project 6: Model Explainability with SHAP

**Objective:**  
Use SHAP values to interpret XGBoost predictions, showing both global and local drivers of attrition.  

---

### ğŸ”‘ Key Steps  
1. **Load & preprocess data** â†’ Dropped leakage columns, encoded categoricals, ensured numeric types.  
2. **Load trained XGBoost model** â†’ From Project 5 (`xgboost_attrition_model.pkl`).  
3. **Subsample data (100 rows)** â†’ Keeps SHAP efficient & avoids kernel crashes.  
4. **Compute SHAP values** â†’ Using new `shap.Explainer` API.  
5. **Generate visuals**:  
   - Global feature importance & summary plots.  
   - Local explanations (waterfall + bar).  
   - Dependence plot for feature interactions.  
6. **Export artifacts** â†’ Saved plots in `/images/` for portfolio clarity.  

---

### ğŸ“ˆ Results  
- **Global Drivers:**  
  - Age, Distance from Home, and Daily Rate (Compensation )are the most influential features.  
- **Local Explanations:**  
  - For an individual employee, SHAP shows exactly which features push their attrition risk up or down.  
- **Feature Interactions:**  
  - Dependence plots reveal how attrition risk rises at lower income bands and with frequent overtime.  
- **Takeaway:**  
  - Moves the model from â€œblack boxâ€ â†’ **explainable AI**, building trust with HR leaders.  

---

### ğŸ”¥ Sample Visuals  

**1. SHAP Summary Plot (Global Drivers)**  
![SHAP Summary Plot](images/shap_summary_plot.png)  
ğŸŒ *Shows which features consistently drive attrition across the workforce.*  

**2. SHAP Feature Importance (Bar Plot)**  
![SHAP Feature Importance](images/shap_feature_importance.png)  
ğŸ“Š *Highlights top features by mean absolute SHAP value â€” OverTime, JobRole, and MonthlyIncome dominate.*  

**3. SHAP Local Explanations (Individual Employee)**  
- Waterfall Plot: ![SHAP Waterfall](images/shap_waterfall_plot.png)  
- Bar Plot: ![SHAP Local Bar](images/shap_local_bar_plot.png)  
ğŸ‘¤ *Explains why one specific employee is predicted as at-risk, showing top contributing features.*  

**4. SHAP Dependence Plot (MonthlyIncome)**  
![SHAP Dependence Plot](images/shap_dependence_plot.png)  
ğŸ”€ *Shows how attrition risk changes with MonthlyIncome while interacting with other features.*  

---

### ğŸ““ Notebook  
- [Attrition_ModelExplainability.ipynb](Attrition_ModelExplainability.ipynb)  
---

### ğŸ“¦ Artifacts  
- [xgboost_attrition_model.pkl](models/xgboost_attrition_model.pkl)  
- Visuals:  
  - [shap_summary_plot.png](images/shap_summary_plot.png)  
  - [shap_feature_importance.png](images/shap_feature_importance.png)  
  - [shap_waterfall_plot.png](images/shap_waterfall_plot.png)  
  - [shap_local_bar_plot.png](images/shap_local_bar_plot.png)  
  - [shap_dependence_plot.png](images/shap_dependence_plot.png)
---
### âœ… Conclusions  
- **Global Drivers:** Overtime, Job Role, and Monthly Income stand out as top predictors.  
- **Local Explanations:** Individual-level SHAP plots build confidence in predictions.  
- **Feature Interactions:** Dependence plots highlight nuanced patterns beyond simple correlations.  

**Why this matters:**  
- SHAP adds transparency â†’ critical for HR applications where fairness & accountability are key.  
- Executives can see not just *who* is at risk, but *why*.  
- Enhances business trust in predictive HR analytics.  

---
## ğŸ“Š Project 7: Interactive Attrition Dashboard (Streamlit)

**Objective:**  
Deploy an interactive web app where HR leaders can upload data, run attrition predictions, and explore explainability visuals.

---

### ğŸ”‘ Key Steps  
1. Built a **Streamlit app (`app.py`)** to host trained models (Logistic, RF, XGBoost).  
2. Created `trained_columns.pkl` for consistent feature alignment during predictions.  
3. Added upload functionality â†’ users can test with their own HR datasets (CSV).  
4. Integrated SHAP for **global & local explanations** inside the dashboard.  
5. Deployed app on **Streamlit Cloud** â†’ shareable public link.  

---

### ğŸ“ˆ Results  
- End-to-end ML workflow is now **live & interactive**.  
- Users can:  
  - Upload data  
  - Select model  
  - View predictions + probabilities  
  - Explore explainability plots  
- Moves this portfolio from **static analysis â†’ deployable HR Tech product**.  

---

### ğŸ–¼ï¸ Sample Dashboard  
![Streamlit Dashboard](images/streamlit_dashboard.png)  

---

### ğŸŒ Live App  
- [Open Dashboard](https://hr-tech-portfolio.streamlit.app/)  

---

### ğŸ“‚ Files  
- [app.py](app.py) â†’ main dashboard app  
- [trained_columns.pkl](models/trained_columns.pkl) â†’ ensures feature alignment  
- [requirements.txt](requirements.txt) â†’ dependencies for deployment  

---

### âœ… Conclusions  
- Streamlit deployment shows the ability to go beyond notebooks and create **real-world HR tools**.  
- Adds credibility for consulting/analytics roles by showcasing full-stack delivery.  
- Sets foundation for future dashboards (Compensation, SQL+ML, etc.).  

---

### ğŸš€ Next Steps  
- Build **SQL + ML pipeline** to query databases + run predictions (Project 8).  
- Extend dashboard with **Compensation Analytics** modules.

## âš’ï¸ Tech Stack  

- Python (Pandas, Matplotlib, Seaborn, scikit-learn, Jupyter Notebook)  
- SQL (SQLite for queries on HR dataset)
- Dataset: [IBM HR Analytics Attrition Dataset (Kaggle)](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)

---

## Model artifacts:(All Models)
- [logistic_attrition_model.pkl](models/logistic_attrition_model.pkl)
- [scaler.pkl](models/scaler.pkl)
- [logistic_attrition_model_tuned.pkl](models/logistic_attrition_model_tuned.pkl)
- [random_forest_attrition_model.pkl](models/random_forest_attrition_model.pkl)
- [random_forest_tuned.pkl](models/random_forest_tuned.pkl)  
- [xgboost_attrition_model.pkl](models/xgboost_attrition_model.pkl)

---

## ğŸ› ï¸ How to Run This Project

Follow these steps to reproduce the analysis on your own system:

1. **Clone the repository**
   ```bash
   git clone https://github.com/AMBOT-pixel96/hr-tech-portfolio.git
   cd hr-tech-portfolio
2. **Create a virtual environment Using Conda:**
```bash
conda create -n hrtech python=3.10 -y
conda activate hrtech
```
3. **Install required packages**
```
pip install -r requirements.txt
```
4. Launch Jupyter Notebook

jupyter notebook


5. Open and run the notebooks

### ğŸ“˜ Baby Steps (Learning Path)
- [Day1-Hello-Amlan.ipynb](notebooks/Day1-Hello-Amlan.ipynb)  
- [Day2-Basics.ipynb](notebooks/Day2-Basics.ipynb)  
- [Day3-DataTypesAndControl.ipynb](notebooks/Day3-DataTypesAndControl.ipynb)  

### ğŸ“Š Main Projects
- [Day4-AttritionRiskAnalyzer.ipynb â†’ Descriptive Analytics](notebooks/Day4-AttritionRiskAnalyzer.ipynb)  
- [Day4-AttritionRiskAnalyzer_v2.0.ipynb â†’ Enhanced Descriptive Analytics](notebooks/Day4-AttritionRiskAnalyzer_v2.0.ipynb)  
- [Attrition_PredictiveModel_V2.ipynb â†’ Predictive Modeling (Logistic Regression)](notebooks/Attrition_PredictiveModel_V2.ipynb)  
- [Attrition_PredictiveModel_V3.ipynb â†’ Predictive Modeling (Tuned Logistic Regression)](notebooks/Attrition_PredictiveModel_V3.ipynb)  
- [Attrition_ModelComparision.ipynb â†’ Logistic Regression vs Random Forest](notebooks/Attrition_ModelComparision.ipynb)  
- [Attrition_AdvancedModels.ipynb â†’ Advanced Models (RF + XGBoost)](notebooks/Attrition_AdvancedModels.ipynb)

---

### ğŸ§¹ Side Quests
-[HR_Data_Cleaning_Utility_V1.ipynb â†’ HR Data Cleanup Utility](sidequests/HR_Data_Cleaning_Utility_V1.ipynb)

---
## ğŸš€ Upcoming Projects  

- Compensation Analytics Dashboard
- SQL query library for HR datasets (attrition by job role, tenure, etc.)  
- Feature engineering + cross-validation for predictive modeling  
- Streamlit dashboard for interactive attrition prediction  

---

## ğŸ§‘â€ğŸ’» About Me  

Iâ€™m exploring the intersection of **Compensation & Benefits, HR Tech, and People Analytics**.  
This repo is my hands-on portfolio â€” tracking progress as I move from HR practitioner â†’ HR Tech consultant.  

---

â­ï¸ If you find this interesting, follow my journey here or connect with me on LinkedIn.

---

<!-- trigger: repo-tree -->

---
